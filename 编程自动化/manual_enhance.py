#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äººå·¥å¢å¼ºå¤„ç†å·¥å…· - Gradio ç•Œé¢
ç”¨äºå¤„ç† LLM å¢å¼ºå¤±è´¥çš„æ¡ç›®ï¼ˆ27æ¡ï¼‰

åŠŸèƒ½ï¼š
  1. æµè§ˆåŸå§‹å›¾ç‰‡ + OCR æ–‡æœ¬
  2. äººå·¥ä¿®æ­£ä¸­è‹±æ–‡æ–‡æœ¬
  3. ç”¨ç©ºè¡Œåˆ†å‰²æ®µè½ï¼Œè‡ªåŠ¨åˆ‡å¥å¯¹é½
  4. ä¿å­˜ç»“æœå¹¶åˆå¹¶åˆ° enhanced_corpus.json

ç”¨æ³•:
  python manual_enhance.py              # å¯åŠ¨å®¡æ ¸ç•Œé¢
  python manual_enhance.py --merge      # å°†æ‰‹åŠ¨ç»“æœåˆå¹¶åˆ°å¢å¼ºè¯­æ–™åº“
  python manual_enhance.py --export     # å¯¼å‡ºå¤±è´¥æ¡ç›®æŠ¥å‘Š

æ“ä½œæŒ‡å—ï¼š
  - æŸ¥çœ‹å·¦ä¾§å›¾ç‰‡ï¼Œå¯¹ç…§å³ä¾§ OCR æ–‡æœ¬
  - ä¿®æ­£ OCR é”™è¯¯ï¼ˆç²˜è¿å•è¯ã€é”™å­—ç­‰ï¼‰
  - ç”¨ä¸€ä¸ªç©ºè¡Œåˆ†éš”ä¸åŒæ®µè½ï¼ˆä¸­è‹±æ–‡åˆ†åˆ«æ“ä½œï¼‰
  - ä¸­æ–‡æ®µè½æ•°å¿…é¡»ç­‰äºè‹±æ–‡æ®µè½æ•°ï¼ˆ1:1 å¯¹é½ï¼‰
  - ç‚¹å‡»"ä¿å­˜å¹¶ä¸‹ä¸€æ¡"è‡ªåŠ¨åˆ‡å¥ã€ä¿å­˜ç»“æœ
  - æ— æ³•ä¿®å¤çš„æ¡ç›®ç‚¹å‡»"è·³è¿‡ï¼ˆæ’é™¤ï¼‰"
"""

import json
import re
import sys
import io
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# ç¼–ç 
if not getattr(sys, '_museum_encoding_set', False):
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys._museum_encoding_set = True
    except Exception:
        pass

sys.path.insert(0, str(Path(__file__).resolve().parent))
import config as cfg


# ==================== æ•°æ®åŠ è½½ ====================

def load_failed_entries() -> List[Dict]:
    """åŠ è½½å¤±è´¥æ¡ç›®ï¼Œè¿”å›å«åŸå§‹æ–‡æœ¬å’Œå›¾ç‰‡è·¯å¾„çš„åˆ—è¡¨"""
    # 1. è¯»å– progress.json è·å– failed åˆ—è¡¨
    if not cfg.ENHANCED_PROGRESS_FILE.exists():
        print("[ERROR] progress.json ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ llm_enhance.py")
        return []

    with open(cfg.ENHANCED_PROGRESS_FILE, 'r', encoding='utf-8') as f:
        progress = json.load(f)

    failed_ids = set(progress.get("failed", []))
    if not failed_ids:
        print("[INFO] æ²¡æœ‰å¤±è´¥æ¡ç›®")
        return []

    # 2. è¯»å– OCR ç»“æœ
    ocr_map = {}
    if cfg.OCR_RESULTS_FILE.exists():
        with open(cfg.OCR_RESULTS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for entry in data.get("results", []):
                image_id = entry.get("image_id", "")
                if image_id in failed_ids:
                    ocr_map[image_id] = entry

    # 3. è¯»å–å®¡æ ¸ç»“æœï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
    review_map = {}
    if cfg.REVIEWED_RESULTS_FILE.exists():
        with open(cfg.REVIEWED_RESULTS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for r in data.get("results", []):
                rid = r.get("image_id", "")
                if rid in failed_ids:
                    review_map[rid] = r

    # 4. è¯»å–å·²æœ‰æ‰‹åŠ¨å¤„ç†ç»“æœ
    manual_file = cfg.ENHANCED_DIR / "manual_results.json"
    manual_done = {}
    if manual_file.exists():
        with open(manual_file, 'r', encoding='utf-8') as f:
            manual_data = json.load(f)
            for entry in manual_data.get("entries", []):
                manual_done[entry["board_id"]] = entry

    # 5. ç»„è£…æ¡ç›®åˆ—è¡¨
    entries = []
    for image_id in sorted(failed_ids):
        ocr = ocr_map.get(image_id, {})
        review = review_map.get(image_id, {})

        # è·å–æ–‡æœ¬ï¼ˆå®¡æ ¸ä¿®æ­£ > åŸå§‹ OCRï¼‰
        zh = ocr.get("zh_text", "")
        en = ocr.get("en_text", "")

        if review.get("review_status") == "corrected":
            czh = review.get("corrected_zh", "")
            cen = review.get("corrected_en", "")
            if czh and czh.strip() != "[åˆ é™¤]":
                zh = czh
            elif czh and czh.strip() == "[åˆ é™¤]":
                zh = ""
            if cen and cen.strip() != "[åˆ é™¤]":
                en = cen
            elif cen and cen.strip() == "[åˆ é™¤]":
                en = ""

        # å›¾ç‰‡è·¯å¾„
        source = ocr.get("source", {})
        museum = source.get("museum", image_id.split("_")[0])
        image_name = source.get("image_name", "")
        if not image_name:
            # ä» image_id æ¨æ–­
            parts = image_id.split("_", 1)
            if len(parts) == 2:
                image_name = parts[1] + ".jpg"
        image_path = cfg.RAW_IMAGE_DIR / museum / image_name

        entry = {
            "image_id": image_id,
            "museum": museum,
            "image_name": image_name,
            "image_path": str(image_path),
            "image_exists": image_path.exists(),
            "zh_text": zh,
            "en_text": en,
            "zh_len": len(zh),
            "en_len": len(en),
            "quality_grade": ocr.get("quality", {}).get("grade", "?"),
            "ocr_confidence": ocr.get("quality", {}).get("confidence", 0),
            "already_done": image_id in manual_done,
            "manual_result": manual_done.get(image_id),
        }
        entries.append(entry)

    return entries


# ==================== æ–‡æœ¬å¤„ç† ====================

def split_paragraphs(text: str) -> List[str]:
    """æŒ‰ç©ºè¡Œåˆ†æ®µ"""
    if not text or not text.strip():
        return []
    # ç”¨è¿ç»­æ¢è¡Œåˆ†å‰²
    paras = re.split(r'\n\s*\n', text.strip())
    return [p.strip() for p in paras if p.strip()]


def split_sentences_zh(text: str) -> List[str]:
    """ä¸­æ–‡åˆ†å¥"""
    if not text.strip():
        return []
    # æŒ‰ ã€‚ï¼ï¼Ÿï¼›åˆ†å¥ï¼Œä¿ç•™æ ‡ç‚¹
    parts = re.split(r'(ã€‚|ï¼|ï¼Ÿ|ï¼›)', text)
    sentences = []
    current = ""
    for part in parts:
        current += part
        if part in ('ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›'):
            if current.strip():
                sentences.append(current.strip())
            current = ""
    if current.strip():
        sentences.append(current.strip())
    return sentences


def split_sentences_en(text: str) -> List[str]:
    """è‹±æ–‡åˆ†å¥"""
    if not text.strip():
        return []
    # å…ˆå¤„ç†å¸¸è§ç¼©å†™é¿å…è¯¯åˆ†
    text_clean = text
    for abbr in ['Mr.', 'Mrs.', 'Dr.', 'Prof.', 'etc.', 'vs.', 'i.e.', 'e.g.', 'No.', 'St.', 'Jr.', 'Sr.', 'Ltd.', 'Corp.', 'Inc.', 'U.S.', 'U.K.', 'B.C.', 'A.D.']:
        text_clean = text_clean.replace(abbr, abbr.replace('.', '<<DOT>>'))

    # æŒ‰ . ! ? åˆ†å¥
    parts = re.split(r'([.!?])\s+', text_clean)
    sentences = []
    current = ""
    for i, part in enumerate(parts):
        current += part
        if part in ('.', '!', '?'):
            restored = current.strip().replace('<<DOT>>', '.')
            if restored:
                sentences.append(restored)
            current = ""
    if current.strip():
        restored = current.strip().replace('<<DOT>>', '.')
        sentences.append(restored)
    return sentences


def align_sentences(zh_sents: List[str], en_sents: List[str]) -> List[Dict]:
    """å¯¹é½ä¸­è‹±å¥å­ï¼ˆå°½é‡ 1:1ï¼Œæ•°é‡ä¸ç­‰æ—¶åˆå¹¶æœ«å°¾ï¼‰"""
    if not zh_sents and not en_sents:
        return []

    pairs = []
    n_zh = len(zh_sents)
    n_en = len(en_sents)

    if n_zh == n_en:
        # å®Œç¾å¯¹é½
        for z, e in zip(zh_sents, en_sents):
            pairs.append({"zh": z, "en": e})
    elif n_zh > n_en:
        # ä¸­æ–‡å¤šï¼Œè‹±æ–‡å°‘ â†’ æœ«å°¾ä¸­æ–‡åˆå¹¶
        for i in range(n_en - 1):
            pairs.append({"zh": zh_sents[i], "en": en_sents[i]})
        # å‰©ä½™ä¸­æ–‡åˆå¹¶åˆ°æœ€åä¸€ä¸ªè‹±æ–‡
        remaining_zh = "".join(zh_sents[n_en - 1:])
        pairs.append({"zh": remaining_zh, "en": en_sents[-1]})
    else:
        # è‹±æ–‡å¤šï¼Œä¸­æ–‡å°‘ â†’ æœ«å°¾è‹±æ–‡åˆå¹¶
        for i in range(n_zh - 1):
            pairs.append({"zh": zh_sents[i], "en": en_sents[i]})
        remaining_en = " ".join(en_sents[n_zh - 1:])
        pairs.append({"zh": zh_sents[-1], "en": remaining_en})

    return pairs


def build_board_from_text(image_id: str, source: Dict,
                          zh_text: str, en_text: str,
                          title_zh: str = "", title_en: str = "") -> Dict:
    """ä»äººå·¥ä¿®æ­£çš„æ–‡æœ¬æ„å»º board ç»“æ„"""
    zh_paras = split_paragraphs(zh_text)
    en_paras = split_paragraphs(en_text)

    # æ®µè½æ•°å¯¹é½ï¼šå¦‚æœä¸ä¸€è‡´ï¼Œå°è¯•åˆå¹¶æœ«å°¾
    if len(zh_paras) != len(en_paras):
        n_min = min(len(zh_paras), len(en_paras))
        if n_min == 0:
            # ä¸€è¾¹å®Œå…¨æ²¡æœ‰æ®µè½
            if not zh_paras:
                zh_paras = [""] * len(en_paras)
            else:
                en_paras = [""] * len(zh_paras)
        else:
            if len(zh_paras) > len(en_paras):
                # åˆå¹¶å¤šä½™ä¸­æ–‡æ®µè½åˆ°æœ€å
                merged = "\n".join(zh_paras[n_min - 1:])
                zh_paras = zh_paras[:n_min - 1] + [merged]
            else:
                merged = " ".join(en_paras[n_min - 1:])
                en_paras = en_paras[:n_min - 1] + [merged]

    paragraphs = []
    for i, (zp, ep) in enumerate(zip(zh_paras, en_paras)):
        zh_sents = split_sentences_zh(zp)
        en_sents = split_sentences_en(ep)
        sent_pairs = align_sentences(zh_sents, en_sents)

        para = {
            "para_index": i,
            "zh": zp,
            "en": ep,
            "sentences": [
                {"sent_index": j, "zh": s["zh"], "en": s["en"]}
                for j, s in enumerate(sent_pairs)
            ],
        }
        paragraphs.append(para)

    return {
        "board_id": image_id,
        "source": source,
        "board_title": {"zh": title_zh, "en": title_en},
        "corrections": {"zh_changes": [], "en_changes": [], "note": "äººå·¥ä¿®æ­£"},
        "paragraphs": paragraphs,
        "manual_processed": True,
        "processed_at": datetime.now().isoformat(),
    }


# ==================== ä¿å­˜ ====================

def save_manual_result(board: Dict):
    """ä¿å­˜ä¸€æ¡æ‰‹åŠ¨å¤„ç†ç»“æœ"""
    manual_file = cfg.ENHANCED_DIR / "manual_results.json"
    cfg.ENHANCED_DIR.mkdir(parents=True, exist_ok=True)

    data = {"metadata": {"updated_at": "", "total": 0}, "entries": []}
    if manual_file.exists():
        with open(manual_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

    # æ›¿æ¢æˆ–è¿½åŠ 
    entries = data.get("entries", [])
    found = False
    for i, e in enumerate(entries):
        if e.get("board_id") == board["board_id"]:
            entries[i] = board
            found = True
            break
    if not found:
        entries.append(board)

    data["entries"] = entries
    data["metadata"]["updated_at"] = datetime.now().isoformat()
    data["metadata"]["total"] = len(entries)

    with open(manual_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return len(entries)


def save_skip(image_id: str):
    """æ ‡è®°è·³è¿‡"""
    manual_file = cfg.ENHANCED_DIR / "manual_results.json"
    cfg.ENHANCED_DIR.mkdir(parents=True, exist_ok=True)

    data = {"metadata": {"updated_at": "", "total": 0}, "entries": [], "skipped": []}
    if manual_file.exists():
        with open(manual_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

    if "skipped" not in data:
        data["skipped"] = []
    if image_id not in data["skipped"]:
        data["skipped"].append(image_id)

    data["metadata"]["updated_at"] = datetime.now().isoformat()

    with open(manual_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


# ==================== åˆå¹¶åˆ°å¢å¼ºè¯­æ–™åº“ ====================

def merge_to_corpus():
    """å°†æ‰‹åŠ¨ç»“æœåˆå¹¶åˆ° enhanced_corpus.json"""
    manual_file = cfg.ENHANCED_DIR / "manual_results.json"
    if not manual_file.exists():
        print("[ERROR] manual_results.json ä¸å­˜åœ¨")
        return

    with open(manual_file, 'r', encoding='utf-8') as f:
        manual_data = json.load(f)

    manual_entries = {e["board_id"]: e for e in manual_data.get("entries", [])}
    manual_skipped = set(manual_data.get("skipped", []))

    if not manual_entries and not manual_skipped:
        print("[INFO] æ²¡æœ‰æ‰‹åŠ¨å¤„ç†ç»“æœ")
        return

    # è¯»å–å¢å¼ºè¯­æ–™åº“
    if not cfg.ENHANCED_CORPUS_FILE.exists():
        print("[ERROR] enhanced_corpus.json ä¸å­˜åœ¨")
        return

    with open(cfg.ENHANCED_CORPUS_FILE, 'r', encoding='utf-8') as f:
        corpus = json.load(f)

    existing_ids = {b["board_id"] for b in corpus.get("boards", [])}

    # æ·»åŠ æ‰‹åŠ¨ç»“æœ
    added = 0
    for board_id, board in manual_entries.items():
        if board_id not in existing_ids:
            corpus["boards"].append(board)
            added += 1
            existing_ids.add(board_id)

    # æ›´æ–°ç»Ÿè®¡
    total_paras = sum(len(b.get("paragraphs", [])) for b in corpus["boards"])
    total_sents = sum(
        len(p.get("sentences", []))
        for b in corpus["boards"]
        for p in b.get("paragraphs", [])
    )

    corpus["metadata"]["total_boards"] = len(corpus["boards"])
    corpus["metadata"]["total_paragraphs"] = total_paras
    corpus["metadata"]["total_sentence_pairs"] = total_sents
    corpus["metadata"]["manual_processed"] = len(manual_entries)
    corpus["metadata"]["manual_skipped"] = len(manual_skipped)
    corpus["metadata"]["merged_at"] = datetime.now().isoformat()

    # æ›´æ–° progress.json ä¸­çš„ failed åˆ—è¡¨ï¼ˆç§»é™¤å·²å¤„ç†çš„ï¼‰
    if cfg.ENHANCED_PROGRESS_FILE.exists():
        with open(cfg.ENHANCED_PROGRESS_FILE, 'r', encoding='utf-8') as f:
            progress = json.load(f)

        old_failed = set(progress.get("failed", []))
        processed = set(manual_entries.keys()) | manual_skipped
        new_failed = [fid for fid in old_failed if fid not in processed]
        progress["failed"] = new_failed
        progress["metadata"]["last_updated"] = datetime.now().isoformat()

        # æŠŠæ‰‹åŠ¨å¤„ç†çš„åŠ å…¥ completed
        for board_id, board in manual_entries.items():
            if board_id not in progress.get("completed", {}):
                progress["completed"][board_id] = {
                    "processed_at": board.get("processed_at", datetime.now().isoformat()),
                    "input_source": "manual",
                    "result": board,
                }
        progress["metadata"]["total_completed"] = len(progress["completed"])

        # æŠŠè·³è¿‡çš„åŠ å…¥ skipped
        for sid in manual_skipped:
            if sid not in progress.get("skipped", []):
                progress["skipped"].append(sid)

        with open(cfg.ENHANCED_PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
        print(f"[OK] progress.json å·²æ›´æ–°: failed {len(old_failed)} â†’ {len(new_failed)}")

    # ä¿å­˜
    with open(cfg.ENHANCED_CORPUS_FILE, 'w', encoding='utf-8') as f:
        json.dump(corpus, f, ensure_ascii=False, indent=2)

    print(f"[OK] å·²åˆå¹¶ {added} æ¡æ‰‹åŠ¨ç»“æœåˆ° enhanced_corpus.json")
    print(f"     è·³è¿‡ {len(manual_skipped)} æ¡")
    print(f"     è¯­æ–™åº“: {corpus['metadata']['total_boards']} å±•æ¿, "
          f"{total_paras} æ®µè½, {total_sents} å¥å¯¹")


# ==================== å¯¼å‡ºæŠ¥å‘Š ====================

def export_report():
    """å¯¼å‡ºå¤±è´¥æ¡ç›®æŠ¥å‘Šï¼ˆæ–¹ä¾¿ç¦»çº¿æŸ¥çœ‹ï¼‰"""
    entries = load_failed_entries()
    if not entries:
        return

    report_file = cfg.ENHANCED_DIR / "failed_entries_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"LLM å¢å¼ºå¤±è´¥æ¡ç›®æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        f.write(f"å…± {len(entries)} æ¡\n")
        f.write("=" * 80 + "\n\n")

        for i, e in enumerate(entries, 1):
            status = "âœ… å·²å¤„ç†" if e["already_done"] else "âŒ å¾…å¤„ç†"
            f.write(f"{'â”€' * 80}\n")
            f.write(f"[{i:02d}] {e['image_id']}  {status}\n")
            f.write(f"     åšç‰©é¦†: {e['museum']}  |  å›¾ç‰‡: {e['image_name']}\n")
            f.write(f"     è´¨é‡: {e['quality_grade']}çº§  |  ç½®ä¿¡åº¦: {e['ocr_confidence']:.2f}\n")
            f.write(f"     ä¸­æ–‡: {e['zh_len']}å­—  |  è‹±æ–‡: {e['en_len']}å­—\n")
            f.write(f"     å›¾ç‰‡: {'å­˜åœ¨' if e['image_exists'] else 'ç¼ºå¤±'} - {e['image_path']}\n")
            f.write(f"\n  ã€ä¸­æ–‡ OCRã€‘\n")
            f.write(f"  {e['zh_text'][:500]}\n")
            if e['zh_len'] > 500:
                f.write(f"  ... (çœç•¥ {e['zh_len'] - 500} å­—)\n")
            f.write(f"\n  ã€è‹±æ–‡ OCRã€‘\n")
            f.write(f"  {e['en_text'][:500]}\n")
            if e['en_len'] > 500:
                f.write(f"  ... (çœç•¥ {e['en_len'] - 500} å­—)\n")
            f.write("\n")

    print(f"[OK] æŠ¥å‘Šå·²å¯¼å‡º: {report_file}")


# ==================== Gradio ç•Œé¢ ====================

def build_gradio_app():
    """æ„å»º Gradio ç•Œé¢"""
    import gradio as gr

    entries = load_failed_entries()
    if not entries:
        print("[ERROR] æ²¡æœ‰éœ€è¦å¤„ç†çš„æ¡ç›®")
        return None

    print(f"[OK] åŠ è½½ {len(entries)} æ¡å¤±è´¥æ¡ç›®")

    # ç»Ÿè®¡
    done_count = sum(1 for e in entries if e["already_done"])
    todo_count = len(entries) - done_count

    current_idx = [0]  # ç”¨åˆ—è¡¨ä½¿å…¶åœ¨é—­åŒ…ä¸­å¯å˜

    def get_entry_info(idx):
        """è·å–å½“å‰æ¡ç›®ä¿¡æ¯"""
        if idx < 0 or idx >= len(entries):
            return None
        return entries[idx]

    def load_entry(idx):
        """åŠ è½½ç¬¬ idx æ¡çš„æ‰€æœ‰ç•Œé¢å…ƒç´ """
        if idx < 0:
            idx = 0
        if idx >= len(entries):
            idx = len(entries) - 1
        current_idx[0] = idx

        e = entries[idx]
        status = "âœ… å·²å¤„ç†" if e["already_done"] else "â³ å¾…å¤„ç†"
        header = (f"**[{idx+1}/{len(entries)}]** `{e['image_id']}`  {status}\n\n"
                  f"åšç‰©é¦†: {e['museum']}  |  è´¨é‡: {e['quality_grade']}çº§  |  "
                  f"ç½®ä¿¡åº¦: {e['ocr_confidence']:.2f}  |  "
                  f"ä¸­æ–‡ {e['zh_len']}å­— / è‹±æ–‡ {e['en_len']}å­—")

        img = e["image_path"] if e["image_exists"] else None

        # å¦‚æœå·²æœ‰æ‰‹åŠ¨ç»“æœï¼ŒåŠ è½½å®ƒ
        zh_text = e["zh_text"]
        en_text = e["en_text"]
        title_zh = ""
        title_en = ""

        if e["manual_result"]:
            mr = e["manual_result"]
            title_zh = mr.get("board_title", {}).get("zh", "")
            title_en = mr.get("board_title", {}).get("en", "")
            # ä» paragraphs é‡å»ºåˆ†æ®µæ–‡æœ¬
            zh_paras = [p.get("zh", "") for p in mr.get("paragraphs", [])]
            en_paras = [p.get("en", "") for p in mr.get("paragraphs", [])]
            if zh_paras:
                zh_text = "\n\n".join(zh_paras)
            if en_paras:
                en_text = "\n\n".join(en_paras)

        return (
            header,   # info_md
            img,      # image
            zh_text,  # zh_textbox
            en_text,  # en_textbox
            title_zh, # title_zh
            title_en, # title_en
            idx,      # slider
        )

    def on_save(zh_text, en_text, title_zh, title_en):
        """ä¿å­˜å½“å‰æ¡ç›®"""
        idx = current_idx[0]
        e = entries[idx]

        zh_paras = split_paragraphs(zh_text)
        en_paras = split_paragraphs(en_text)

        # éªŒè¯
        if not zh_text.strip() and not en_text.strip():
            return 'âš ï¸ ä¸­è‹±æ–‡éƒ½ä¸ºç©ºï¼Œè¯·å¡«å†™è‡³å°‘ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ï¼Œæˆ–ç‚¹å‡»ã€Œè·³è¿‡ã€'

        if zh_paras and en_paras and len(zh_paras) != len(en_paras):
            return (f"âš ï¸ æ®µè½æ•°ä¸åŒ¹é…ï¼šä¸­æ–‡ {len(zh_paras)} æ®µ vs è‹±æ–‡ {len(en_paras)} æ®µ\n"
                    f"è¯·ç”¨ç©ºè¡Œåˆ†éš”æ®µè½ï¼Œç¡®ä¿ä¸­è‹±æ–‡æ®µè½æ•°ä¸€è‡´ã€‚\n"
                    f"ï¼ˆå¦‚æœæ— æ³•å¯¹é½ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨åˆå¹¶æœ«å°¾æ®µè½ï¼‰")

        # æ„å»º board
        source = {
            "museum": e["museum"],
            "image_name": e["image_name"],
            "image_path": e["image_path"],
        }
        board = build_board_from_text(
            e["image_id"], source,
            zh_text, en_text,
            title_zh.strip(), title_en.strip()
        )

        n_para = len(board["paragraphs"])
        n_sent = sum(len(p["sentences"]) for p in board["paragraphs"])

        count = save_manual_result(board)
        entries[idx]["already_done"] = True
        entries[idx]["manual_result"] = board

        return (f"âœ… å·²ä¿å­˜ `{e['image_id']}`\n"
                f"   {n_para} æ®µè½, {n_sent} å¥å¯¹\n"
                f"   æ€»è®¡å·²å¤„ç†: {count} æ¡")

    def on_skip():
        """è·³è¿‡å½“å‰æ¡ç›®"""
        idx = current_idx[0]
        e = entries[idx]
        save_skip(e["image_id"])
        entries[idx]["already_done"] = True
        return f"â­ï¸ å·²è·³è¿‡ `{e['image_id']}`"

    def on_preview(zh_text, en_text, title_zh, title_en):
        """é¢„è§ˆåˆ†æ®µåˆ†å¥ç»“æœ"""
        zh_paras = split_paragraphs(zh_text)
        en_paras = split_paragraphs(en_text)

        lines = []
        lines.append(f"**æ ‡é¢˜**: zh=ã€Œ{title_zh.strip()}ã€ en=ã€Œ{title_en.strip()}ã€\n")
        lines.append(f"**æ®µè½æ•°**: ä¸­æ–‡ {len(zh_paras)} æ®µ, è‹±æ–‡ {len(en_paras)} æ®µ\n")

        if zh_paras and en_paras and len(zh_paras) != len(en_paras):
            lines.append(f"âš ï¸ **æ®µè½æ•°ä¸ä¸€è‡´ï¼** ä¿å­˜æ—¶ä¼šè‡ªåŠ¨åˆå¹¶æœ«å°¾æ®µè½\n")

        # æ¨¡æ‹Ÿå¯¹é½
        n = max(len(zh_paras), len(en_paras))
        for i in range(n):
            lines.append(f"---\n### æ®µè½ {i+1}")
            zp = zh_paras[i] if i < len(zh_paras) else "(æ— )"
            ep = en_paras[i] if i < len(en_paras) else "(æ— )"

            zh_sents = split_sentences_zh(zp) if zp != "(æ— )" else []
            en_sents = split_sentences_en(ep) if ep != "(æ— )" else []
            pairs = align_sentences(zh_sents, en_sents) if zh_sents or en_sents else []

            lines.append(f"å¥å¯¹æ•°: {len(pairs)}\n")
            for j, pair in enumerate(pairs):
                lines.append(f"**S{j+1}** ZH: {pair['zh']}")
                lines.append(f"**S{j+1}** EN: {pair['en']}\n")

        return "\n".join(lines)

    def go_next():
        idx = min(current_idx[0] + 1, len(entries) - 1)
        return load_entry(idx)

    def go_prev():
        idx = max(current_idx[0] - 1, 0)
        return load_entry(idx)

    def go_to(idx):
        return load_entry(int(idx))

    # ---------- æ„å»ºç•Œé¢ ----------
    with gr.Blocks(
        title="äººå·¥å¢å¼ºå¤„ç†",
        theme=gr.themes.Soft(),
    ) as app:
        gr.Markdown(
            f"# ğŸ“ äººå·¥å¢å¼ºå¤„ç†å·¥å…·\n"
            f"å…± **{len(entries)}** æ¡å¤±è´¥æ¡ç›® | "
            f"å·²å¤„ç† **{done_count}** | å¾…å¤„ç† **{todo_count}**\n\n"
            f"æ“ä½œï¼šä¿®æ­£æ–‡æœ¬ â†’ ç”¨**ç©ºè¡Œ**åˆ†æ®µ â†’ é¢„è§ˆ â†’ ä¿å­˜"
        )

        with gr.Row():
            prev_btn = gr.Button("â¬…ï¸ ä¸Šä¸€æ¡", scale=1)
            slider = gr.Slider(
                minimum=0, maximum=len(entries) - 1, step=1, value=0,
                label="æ¡ç›®ç´¢å¼•", scale=4,
            )
            next_btn = gr.Button("â¡ï¸ ä¸‹ä¸€æ¡", scale=1)

        info_md = gr.Markdown("")

        with gr.Row(equal_height=True):
            with gr.Column(scale=1):
                image = gr.Image(label="åŸå§‹å›¾ç‰‡", type="filepath", height=500)

            with gr.Column(scale=1):
                with gr.Row():
                    title_zh_box = gr.Textbox(label="æ ‡é¢˜(ä¸­)", placeholder="å¯ç•™ç©º", scale=1)
                    title_en_box = gr.Textbox(label="æ ‡é¢˜(è‹±)", placeholder="å¯ç•™ç©º", scale=1)
                zh_box = gr.Textbox(
                    label="ä¸­æ–‡æ–‡æœ¬ï¼ˆç”¨ç©ºè¡Œåˆ†æ®µï¼‰",
                    lines=12, max_lines=30,
                    placeholder="ä¿®æ­£ OCR é”™è¯¯åç²˜è´´ä¸­æ–‡æ–‡æœ¬...\n\nç”¨ç©ºè¡Œåˆ†éš”ä¸åŒæ®µè½",
                )
                en_box = gr.Textbox(
                    label="è‹±æ–‡æ–‡æœ¬ï¼ˆç”¨ç©ºè¡Œåˆ†æ®µï¼‰",
                    lines=12, max_lines=30,
                    placeholder="ä¿®æ­£ OCR é”™è¯¯åç²˜è´´è‹±æ–‡æ–‡æœ¬...\n\nç”¨ç©ºè¡Œåˆ†éš”ä¸åŒæ®µè½",
                )

        with gr.Row():
            save_btn = gr.Button("ğŸ’¾ ä¿å­˜å¹¶å¤„ç†", variant="primary", scale=2)
            preview_btn = gr.Button("ğŸ‘ï¸ é¢„è§ˆåˆ†æ®µåˆ†å¥", scale=1)
            skip_btn = gr.Button("â­ï¸ è·³è¿‡ï¼ˆæ’é™¤æ­¤æ¡ï¼‰", variant="stop", scale=1)

        status_md = gr.Markdown("")
        preview_md = gr.Markdown("")

        # è¾“å‡ºç»„ä»¶åˆ—è¡¨
        load_outputs = [info_md, image, zh_box, en_box, title_zh_box, title_en_box, slider]

        # äº‹ä»¶ç»‘å®š
        slider.change(go_to, inputs=[slider], outputs=load_outputs)
        prev_btn.click(go_prev, outputs=load_outputs)
        next_btn.click(go_next, outputs=load_outputs)

        save_btn.click(
            on_save,
            inputs=[zh_box, en_box, title_zh_box, title_en_box],
            outputs=[status_md],
        )
        skip_btn.click(on_skip, outputs=[status_md])
        preview_btn.click(
            on_preview,
            inputs=[zh_box, en_box, title_zh_box, title_en_box],
            outputs=[preview_md],
        )

        # åˆå§‹åŠ è½½
        app.load(lambda: load_entry(0), outputs=load_outputs)

    return app


# ==================== ä¸»å…¥å£ ====================

def main():
    parser = argparse.ArgumentParser(description="äººå·¥å¢å¼ºå¤„ç†å·¥å…·")
    parser.add_argument("--merge", action="store_true",
                        help="å°†æ‰‹åŠ¨ç»“æœåˆå¹¶åˆ° enhanced_corpus.json")
    parser.add_argument("--export", action="store_true",
                        help="å¯¼å‡ºå¤±è´¥æ¡ç›®æŠ¥å‘Š")
    parser.add_argument("--port", type=int, default=7866,
                        help="Gradio æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤7866ï¼‰")
    args = parser.parse_args()

    if args.merge:
        merge_to_corpus()
        return

    if args.export:
        export_report()
        return

    app = build_gradio_app()
    if app:
        print(f"\nå¯åŠ¨äººå·¥å¢å¼ºç•Œé¢: http://127.0.0.1:{args.port}")
        print("æ“ä½œå®Œæˆåè¿è¡Œ: python manual_enhance.py --merge")
        app.launch(
            server_name="127.0.0.1",
            server_port=args.port,
            show_error=True,
        )


if __name__ == "__main__":
    main()
